{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62114b3-d304-4753-9670-ffdb582890a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/latest/hand_landmarker.task\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc477e4-32dd-43b9-8730-af51c8f63c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "lm_result = None\n",
    "def print_result(result: mp.tasks.vision.HandLandmarkerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    global lm_result\n",
    "    lm_result = result\n",
    "    # print(f'hand landmarker result: {result}') \n",
    "\n",
    "def detect_async(frame, landmarker):\n",
    "      # convert np frame to mp image\n",
    "      mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "      # detect landmarks\n",
    "      landmarker.detect_async(image = mp_image, timestamp_ms = int(time.time() * 1000))\n",
    "\n",
    "def close(landmarker):\n",
    "  # close landmarker\n",
    "  landmarker.close()\n",
    "\n",
    "model_path = 'hand_landmarker.task'\n",
    "\n",
    "options = mp.tasks.vision.HandLandmarkerOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,\n",
    "    min_hand_detection_confidence = 0.3, # lower than value to get predictions more often\n",
    "    min_hand_presence_confidence = 0.3, # lower than value to get predictions more often\n",
    "    min_tracking_confidence = 0.3, # lower than value to get predictions more often\n",
    "    num_hands=2,\n",
    "    result_callback=print_result)\n",
    "\n",
    "# options for landmarker in image mode:\n",
    "options_image = mp.tasks.vision.HandLandmarkerOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=mp.tasks.vision.RunningMode.IMAGE,\n",
    "    min_hand_detection_confidence = 0.2,\n",
    "    min_hand_presence_confidence = 0.2,\n",
    "    min_tracking_confidence = 0.2,\n",
    "    # num_hands=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf3ba1c-8438-493d-84b0-ebf314883b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result: mp.tasks.vision.HandLandmarkerResult):\n",
    "    \"\"\"Courtesy of https://github.com/googlesamples/mediapipe/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb\"\"\"\n",
    "    global first\n",
    "    try:\n",
    "        if detection_result.hand_landmarks == []:\n",
    "            return rgb_image\n",
    "        else:\n",
    "            hand_landmarks_list = detection_result.hand_landmarks\n",
    "            annotated_image = np.copy(rgb_image)\n",
    "            \n",
    "            # Loop through the detected hands to visualize.\n",
    "            for idx in range(len(hand_landmarks_list)):\n",
    "                hand_landmarks = hand_landmarks_list[idx]\n",
    "                # Draw the hand landmarks.\n",
    "                hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "                hand_landmarks_proto.landmark.extend([\n",
    "                    landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks])\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    annotated_image,\n",
    "                    hand_landmarks_proto,\n",
    "                    mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                    mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp.solutions.drawing_styles.get_default_hand_connections_style())\n",
    "                \n",
    "            return annotated_image\n",
    "    except Exception as e:\n",
    "        print(e, 'exception')\n",
    "        return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7b8dd0-0671-43da-bcc2-7f5041c963df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coord_list(detected_landmarks):\n",
    "  coord_list = []\n",
    "  for i in detected_landmarks:\n",
    "    coord_list.append([i.x, i.y, i.z])\n",
    "  return coord_list\n",
    "    \n",
    "def orthonormal_basis_from_triangle(p1, p2, p3):\n",
    "    \"\"\"Return orthonormal triad (e1,e2,e3): e1 along (p2-p1), e2 in the plane, e3 = normal.\"\"\"\n",
    "    u = p2 - p1\n",
    "    v = p3 - p1\n",
    "    n = np.cross(u, v)\n",
    "    if np.linalg.norm(n) < 1e-12:\n",
    "        raise ValueError(\"Anchor points are collinear; cannot define a plane.\")\n",
    "    e1 = u / np.linalg.norm(u)\n",
    "    v_perp = v - np.dot(v, e1) * e1\n",
    "    e2 = v_perp / np.linalg.norm(v_perp)\n",
    "    e3 = np.cross(e1, e2)  # already unit\n",
    "    return e1, e2, e3\n",
    "\n",
    "def solve_affine2d(src_xy, dst_xy):\n",
    "    \"\"\"Solve 2D affine mapping (x,y)->(X,Y) from 3 point pairs. Returns 2x2 M and 2-dim t.\"\"\"\n",
    "    A, b = [], []\n",
    "    for (x, y), (X, Y) in zip(src_xy, dst_xy):\n",
    "        A.append([x, y, 1, 0, 0, 0])\n",
    "        A.append([0, 0, 0, x, y, 1])\n",
    "        b += [X, Y]\n",
    "    A = np.asarray(A, float)\n",
    "    b = np.asarray(b, float)\n",
    "    m11, m12, tx, m21, m22, ty = np.linalg.solve(A, b)\n",
    "    M = np.array([[m11, m12],\n",
    "                  [m21, m22]], float)\n",
    "    t = np.array([tx, ty], float)\n",
    "    return M, t\n",
    "\n",
    "def tilt_from_z_original(P, idx0=0, idx1=17, degrees=False):\n",
    "    \"\"\"\n",
    "    Angle between the original vector P[idx1]-P[idx0] and the global z-axis.\n",
    "    Returns angle in radians\n",
    "    \"\"\"\n",
    "    v = np.asarray(P[idx1]) - np.asarray(P[idx0])\n",
    "    n = np.linalg.norm(v)\n",
    "    if n < 1e-12:\n",
    "        return np.nan  # or raise ValueError(\"Zero-length vector\")\n",
    "    v_unit = v / n\n",
    "    cos_theta = np.clip(v_unit[2], -1.0, 1.0)   # dot(v̂, ẑ) = z-component\n",
    "    theta = np.arccos(cos_theta)                # 0 = vertical, pi/2 = horizontal\n",
    "    return theta\n",
    "\n",
    "def normalize_landmarks(points, anchor_idx=(0,17,5), lambda_normal=1.0):\n",
    "    \"\"\"\n",
    "    Build a 3D affine transform (A,t) so that:\n",
    "      P[i1] -> (0,0,0), P[i2] -> (0,1,0), P[i3] -> (1,1,0),\n",
    "    and the plane normal maps to the Z axis with scale lambda_normal.\n",
    "    Returns transformed points (N,3) and (A,t).\n",
    "    \"\"\"\n",
    "    P = np.asarray(points, float)\n",
    "    i1, i2, i3 = anchor_idx\n",
    "    p1, p2, p3 = P[i1], P[i2], P[i3]\n",
    "\n",
    "    # Local orthonormal basis B = [e1 e2 e3]\n",
    "    e1, e2, e3 = orthonormal_basis_from_triangle(p1, p2, p3)\n",
    "    B = np.column_stack([e1, e2, e3])          # world <- local\n",
    "    BT = B.T                                    # local <- world\n",
    "\n",
    "    # Local coordinates of anchors (q = BT * (p - p1))\n",
    "    q1 = BT @ (p1 - p1)                         # ~ (0,0,0)\n",
    "    q2 = BT @ (p2 - p1)                         # (x2, y2, 0)\n",
    "    q3 = BT @ (p3 - p1)                         # (x3, y3, 0)\n",
    "\n",
    "    src_xy = np.vstack([q1[:2], q2[:2], q3[:2]])\n",
    "    dst_xy = np.array([[0.,0.],[0.,1.],[1.,1.]], float)\n",
    "\n",
    "    # Exact 2D affine that hits the three targets in XY\n",
    "    M2, t2 = solve_affine2d(src_xy, dst_xy)\n",
    "\n",
    "    # Compose full 3x3 local linear map: XY via M2, Z via lambda_normal\n",
    "    L_local = np.array([[M2[0,0], M2[0,1], 0.0],\n",
    "                        [M2[1,0], M2[1,1], 0.0],\n",
    "                        [0.0,      0.0,    lambda_normal]], float)\n",
    "\n",
    "    # Because q1 = (0,0,0) maps to (0,0,0), t2 should be ~0; keep it for completeness\n",
    "    t_local = np.array([t2[0], t2[1], 0.0], float)\n",
    "\n",
    "    # Convert to a single world-space affine: P' = A @ P + t\n",
    "    # q = BT @ (P - p1);  q' = L_local @ q + t_local;  P' = q'  (targets are in world axes)\n",
    "    A = L_local @ BT\n",
    "    t = -L_local @ (BT @ p1) + t_local\n",
    "\n",
    "    # Apply to all points\n",
    "    normalized_points = (A @ P.T).T + t\n",
    "\n",
    "    return normalized_points, tilt_from_z_original(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae33056c-4120-42cb-b9b5-c859a44f9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_result(result, clazz, normalize = False):\n",
    "    d = {}\n",
    "    if normalize:\n",
    "        coords = create_coord_list(result.hand_landmarks[0])\n",
    "        norm_coords, tilt = normalize_landmarks(coords)\n",
    "        for k in range(len(coords)):\n",
    "            d[str(k)+'_x'] = norm_coords[k][0]\n",
    "            d[str(k)+'_y'] = norm_coords[k][1]\n",
    "            d[str(k)+'_z'] = norm_coords[k][2]\n",
    "        d['tilt'] = tilt\n",
    "    \n",
    "    else:\n",
    "        for k,v in enumerate(result.hand_landmarks[0]):\n",
    "            d[str(k)+'_x'] = v.x\n",
    "            d[str(k)+'_y'] = v.y\n",
    "            d[str(k)+'_z'] = v.z\n",
    "    \n",
    "    if clazz != None:    \n",
    "        d['clazz'] = clazz\n",
    "    # print(d)\n",
    "    return d\n",
    "    \n",
    "    # store result values in dict and add to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e3d5fa-7dcc-40fc-98a1-44b1a76deb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of dicts to pd.df\n",
    "def get_data_frame(result_list):\n",
    "    return pd.DataFrame(result_list)\n",
    "\n",
    "def get_keypoints(landmarker, base_path, file_name, clazz):\n",
    "    image = mp.Image.create_from_file(base_path + file_name)\n",
    "    return detect(image, clazz, landmarker)\n",
    "\n",
    "def detect(image, clazz, landmarker):\n",
    "    return (landmarker.detect(image=image), clazz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73f1af1-f6de-4388-a04c-51b65c8be9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas==2.1.4 moet numpy <2 gebruiken voor mediapipe!\n",
    "import pandas as pd\n",
    "\n",
    "# create the landmarker\n",
    "landmarker_image = mp.tasks.vision.HandLandmarker.create_from_options(options_image)\n",
    "\n",
    "data_path = './../data/American Sign Language Letters.v1-v1.tensorflow/'\n",
    "\n",
    "train_data = pd.read_csv(data_path + 'train/_annotations.csv')\n",
    "train_data.head()\n",
    "\n",
    "valid_data = pd.read_csv(data_path + 'valid/_annotations.csv')\n",
    "valid_data.head()\n",
    "\n",
    "test_data = pd.read_csv(data_path + 'test/_annotations.csv')\n",
    "test_data.head()\n",
    "\n",
    "train_data[:1]\n",
    "counter = 0\n",
    "    \n",
    "def store_data(data, folder, normalize=False):\n",
    "    result_list = []\n",
    "    # loop through images in csv and get results\n",
    "    for i in range(len(data)):\n",
    "        values = data[i:i+1][['filename','class']].values[0]\n",
    "        file_name, clazz = values[0], values[1]\n",
    "        result = get_keypoints(landmarker_image, data_path + folder, file_name, clazz)\n",
    "        if len(result[0].hand_landmarks) > 0:\n",
    "            result_list.append(store_result(result[0], clazz, normalize))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2267ea-5ab5-45d6-bef5-d06b0213f69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_x</th>\n",
       "      <th>0_y</th>\n",
       "      <th>0_z</th>\n",
       "      <th>1_x</th>\n",
       "      <th>1_y</th>\n",
       "      <th>1_z</th>\n",
       "      <th>2_x</th>\n",
       "      <th>2_y</th>\n",
       "      <th>2_z</th>\n",
       "      <th>3_x</th>\n",
       "      <th>...</th>\n",
       "      <th>18_y</th>\n",
       "      <th>18_z</th>\n",
       "      <th>19_x</th>\n",
       "      <th>19_y</th>\n",
       "      <th>19_z</th>\n",
       "      <th>20_x</th>\n",
       "      <th>20_y</th>\n",
       "      <th>20_z</th>\n",
       "      <th>tilt</th>\n",
       "      <th>clazz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.440892e-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.327120</td>\n",
       "      <td>0.090737</td>\n",
       "      <td>0.045319</td>\n",
       "      <td>0.626213</td>\n",
       "      <td>0.228807</td>\n",
       "      <td>0.075154</td>\n",
       "      <td>0.607811</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275446</td>\n",
       "      <td>0.019063</td>\n",
       "      <td>-0.188385</td>\n",
       "      <td>1.415672</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>-0.239304</td>\n",
       "      <td>1.589045</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>1.935054</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.163336e-17</td>\n",
       "      <td>5.551115e-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525119</td>\n",
       "      <td>0.206240</td>\n",
       "      <td>-0.071910</td>\n",
       "      <td>0.828477</td>\n",
       "      <td>0.543234</td>\n",
       "      <td>-0.132616</td>\n",
       "      <td>0.932525</td>\n",
       "      <td>...</td>\n",
       "      <td>1.177242</td>\n",
       "      <td>-0.129969</td>\n",
       "      <td>0.229686</td>\n",
       "      <td>1.037169</td>\n",
       "      <td>-0.145758</td>\n",
       "      <td>0.188796</td>\n",
       "      <td>0.954804</td>\n",
       "      <td>-0.093303</td>\n",
       "      <td>2.952533</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>-4.440892e-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444774</td>\n",
       "      <td>0.216007</td>\n",
       "      <td>-0.078405</td>\n",
       "      <td>0.730040</td>\n",
       "      <td>0.448115</td>\n",
       "      <td>-0.118868</td>\n",
       "      <td>0.519361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901946</td>\n",
       "      <td>-0.066201</td>\n",
       "      <td>-0.210894</td>\n",
       "      <td>0.662981</td>\n",
       "      <td>-0.085913</td>\n",
       "      <td>-0.107566</td>\n",
       "      <td>0.661150</td>\n",
       "      <td>-0.064426</td>\n",
       "      <td>2.187825</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>4.440892e-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.897693</td>\n",
       "      <td>0.348256</td>\n",
       "      <td>-0.091335</td>\n",
       "      <td>1.310136</td>\n",
       "      <td>0.783704</td>\n",
       "      <td>-0.194932</td>\n",
       "      <td>1.397869</td>\n",
       "      <td>...</td>\n",
       "      <td>1.469281</td>\n",
       "      <td>-0.072216</td>\n",
       "      <td>0.291336</td>\n",
       "      <td>1.651877</td>\n",
       "      <td>-0.135644</td>\n",
       "      <td>0.358670</td>\n",
       "      <td>1.770191</td>\n",
       "      <td>-0.184458</td>\n",
       "      <td>1.861912</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386620</td>\n",
       "      <td>0.130031</td>\n",
       "      <td>-0.047090</td>\n",
       "      <td>0.742501</td>\n",
       "      <td>0.262637</td>\n",
       "      <td>-0.082818</td>\n",
       "      <td>0.643822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670696</td>\n",
       "      <td>-0.064583</td>\n",
       "      <td>-0.138432</td>\n",
       "      <td>0.604676</td>\n",
       "      <td>-0.066883</td>\n",
       "      <td>-0.059686</td>\n",
       "      <td>0.703596</td>\n",
       "      <td>-0.051676</td>\n",
       "      <td>2.247806</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0_x           0_y  0_z       1_x       1_y       1_z       2_x  \\\n",
       "0  0.000000e+00  4.440892e-16  0.0  0.327120  0.090737  0.045319  0.626213   \n",
       "1  4.163336e-17  5.551115e-17  0.0  0.525119  0.206240 -0.071910  0.828477   \n",
       "2 -2.220446e-16 -4.440892e-16  0.0  0.444774  0.216007 -0.078405  0.730040   \n",
       "3  1.110223e-16  4.440892e-16  0.0  0.897693  0.348256 -0.091335  1.310136   \n",
       "4 -2.220446e-16  0.000000e+00  0.0  0.386620  0.130031 -0.047090  0.742501   \n",
       "\n",
       "        2_y       2_z       3_x  ...      18_y      18_z      19_x      19_y  \\\n",
       "0  0.228807  0.075154  0.607811  ...  1.275446  0.019063 -0.188385  1.415672   \n",
       "1  0.543234 -0.132616  0.932525  ...  1.177242 -0.129969  0.229686  1.037169   \n",
       "2  0.448115 -0.118868  0.519361  ...  0.901946 -0.066201 -0.210894  0.662981   \n",
       "3  0.783704 -0.194932  1.397869  ...  1.469281 -0.072216  0.291336  1.651877   \n",
       "4  0.262637 -0.082818  0.643822  ...  0.670696 -0.064583 -0.138432  0.604676   \n",
       "\n",
       "       19_z      20_x      20_y      20_z      tilt  clazz  \n",
       "0  0.016589 -0.239304  1.589045  0.002881  1.935054      J  \n",
       "1 -0.145758  0.188796  0.954804 -0.093303  2.952533      Q  \n",
       "2 -0.085913 -0.107566  0.661150 -0.064426  2.187825      Z  \n",
       "3 -0.135644  0.358670  1.770191 -0.184458  1.861912      R  \n",
       "4 -0.066883 -0.059686  0.703596 -0.051676  2.247806      Z  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load normalized results into DataFrame\n",
    "df_train_normalized = get_data_frame(store_data(train_data, 'train/', True))\n",
    "df_train_normalized.head()\n",
    "\n",
    "df_valid_normalized = get_data_frame(store_data(valid_data, 'valid/', True))\n",
    "df_valid_normalized.head()\n",
    "\n",
    "df_test_normalized = get_data_frame(store_data(test_data, 'test/', True))\n",
    "df_test_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68867de5-4eab-448d-b14e-ae7da877a96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics: Training\n",
      "accuracy: 0.9622\n",
      "f1: 0.9624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.89      0.97      0.93        64\n",
      "           B       1.00      1.00      1.00        35\n",
      "           C       0.94      0.97      0.96        33\n",
      "           D       0.92      0.88      0.90        50\n",
      "           E       0.98      0.92      0.95        51\n",
      "           F       1.00      0.98      0.99        53\n",
      "           G       0.92      0.98      0.95        49\n",
      "           H       1.00      0.97      0.99        36\n",
      "           I       0.99      0.99      0.99        71\n",
      "           J       0.90      0.90      0.90        71\n",
      "           K       1.00      0.96      0.98        45\n",
      "           L       1.00      0.98      0.99        64\n",
      "           M       1.00      0.96      0.98        45\n",
      "           N       0.96      0.96      0.96        53\n",
      "           O       0.90      0.98      0.94        47\n",
      "           P       0.97      0.97      0.97        39\n",
      "           Q       1.00      0.98      0.99        49\n",
      "           R       0.98      0.95      0.96        43\n",
      "           S       0.97      0.98      0.97        59\n",
      "           T       0.95      0.98      0.96        41\n",
      "           U       1.00      1.00      1.00        43\n",
      "           V       0.94      0.94      0.94        47\n",
      "           W       1.00      0.96      0.98        49\n",
      "           X       0.95      0.93      0.94        56\n",
      "           Y       0.98      0.98      0.98        43\n",
      "           Z       0.97      0.98      0.98        62\n",
      "\n",
      "    accuracy                           0.96      1298\n",
      "   macro avg       0.97      0.96      0.96      1298\n",
      "weighted avg       0.96      0.96      0.96      1298\n",
      "\n",
      "Statistics: Validation\n",
      "accuracy: 0.8162\n",
      "f1: 0.8221\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.80      0.80      0.80         5\n",
      "           B       1.00      0.67      0.80         9\n",
      "           C       0.25      0.50      0.33         2\n",
      "           D       0.80      0.80      0.80         5\n",
      "           E       1.00      0.75      0.86         4\n",
      "           F       1.00      1.00      1.00         8\n",
      "           G       0.83      1.00      0.91         5\n",
      "           H       1.00      0.89      0.94         9\n",
      "           I       0.00      0.00      0.00         2\n",
      "           J       0.86      0.86      0.86         7\n",
      "           K       0.83      0.83      0.83         6\n",
      "           L       0.80      1.00      0.89         4\n",
      "           M       1.00      0.71      0.83         7\n",
      "           N       1.00      0.67      0.80         3\n",
      "           O       1.00      1.00      1.00         4\n",
      "           P       1.00      0.71      0.83         7\n",
      "           Q       0.67      0.50      0.57         4\n",
      "           R       0.83      0.71      0.77         7\n",
      "           S       0.80      1.00      0.89         4\n",
      "           T       0.71      0.83      0.77         6\n",
      "           U       0.75      0.86      0.80         7\n",
      "           V       0.67      0.80      0.73         5\n",
      "           W       0.43      1.00      0.60         3\n",
      "           X       0.00      0.00      0.00         1\n",
      "           Y       1.00      1.00      1.00         8\n",
      "           Z       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.82       136\n",
      "   macro avg       0.77      0.77      0.75       136\n",
      "weighted avg       0.85      0.82      0.82       136\n",
      "\n",
      "Statistics: Test\n",
      "accuracy: 0.8788\n",
      "f1: 0.8702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.50      1.00      0.67         1\n",
      "           B       0.75      1.00      0.86         3\n",
      "           C       0.80      1.00      0.89         4\n",
      "           D       1.00      1.00      1.00         1\n",
      "           F       1.00      1.00      1.00         2\n",
      "           G       1.00      1.00      1.00         4\n",
      "           H       1.00      1.00      1.00         2\n",
      "           I       1.00      1.00      1.00         2\n",
      "           J       0.80      1.00      0.89         4\n",
      "           K       1.00      1.00      1.00         3\n",
      "           M       1.00      0.33      0.50         3\n",
      "           N       0.50      1.00      0.67         1\n",
      "           O       1.00      1.00      1.00         3\n",
      "           P       1.00      1.00      1.00         1\n",
      "           Q       1.00      0.50      0.67         2\n",
      "           R       1.00      0.50      0.67         2\n",
      "           S       1.00      1.00      1.00         3\n",
      "           T       1.00      1.00      1.00         5\n",
      "           U       0.67      1.00      0.80         2\n",
      "           V       0.75      0.75      0.75         4\n",
      "           W       1.00      0.50      0.67         4\n",
      "           X       0.80      1.00      0.89         4\n",
      "           Y       1.00      1.00      1.00         2\n",
      "           Z       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.88        66\n",
      "   macro avg       0.90      0.89      0.87        66\n",
      "weighted avg       0.91      0.88      0.87        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# training features\n",
    "X_features = ['0_x','0_y','0_z','1_x','1_y','1_z','2_x','2_y','2_z'\n",
    "                    ,'3_x','3_y','3_z','4_x','4_y','4_z','5_x','5_y','5_z'\n",
    "                    ,'6_x','6_y','6_z','7_x','7_y','7_z','8_x','8_y','8_z'\n",
    "                    ,'9_x','9_y','9_z','10_x','10_y','10_z','11_x','11_y','11_z'\n",
    "                    ,'12_x','12_y','12_z','13_x','13_y','13_z','14_x','14_y','14_z'\n",
    "                    ,'15_x','15_y','15_z','16_x','16_y','16_z','17_x','17_y','17_z'\n",
    "                    ,'18_x','18_y','18_z','19_x','19_y','19_z','20_x','20_y','20_z', 'tilt'\n",
    "]\n",
    "\n",
    "y_target = 'clazz'\n",
    "\n",
    "# prepare data for training\n",
    "df_training_normalized_x = df_train_normalized[X_features]\n",
    "df_valid_normalized_x = df_valid_normalized[X_features]\n",
    "df_test_normalized_x = df_test_normalized[X_features]\n",
    "\n",
    "df_training_normalized_y = df_train_normalized[y_target]\n",
    "df_valid_normalized_y = df_valid_normalized[y_target]\n",
    "df_test_normalized_y = df_test_normalized[y_target]\n",
    "\n",
    "def train_nn_model(X_train, y_train):\n",
    "    clf = MLPClassifier(\n",
    "        solver='adam', \n",
    "        alpha=1e-5,\n",
    "        hidden_layer_sizes=(26, 26), \n",
    "        random_state=42,\n",
    "        max_iter=2000\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def test_nn_model(model, title, X, y):\n",
    "    # y_pred = model.predict_proba(X_test)[:,1]\n",
    "    print(f'Statistics: {title}')\n",
    "    print('accuracy: {:5.4f}'.format(accuracy_score(y_true=y, y_pred=model.predict(X=X))))\n",
    "    print('f1: {:5.4f}'.format(f1_score(average='weighted', y_true=y, y_pred=model.predict(X=X))))\n",
    "    print(classification_report(y_true=y, y_pred=model.predict(X=X)))\n",
    "\n",
    "nn_model = train_nn_model(df_training_normalized_x, df_training_normalized_y)\n",
    "test_nn_model(nn_model, 'Training', df_training_normalized_x, df_training_normalized_y)\n",
    "test_nn_model(nn_model, 'Validation', df_valid_normalized_x, df_valid_normalized_y)\n",
    "test_nn_model(nn_model, 'Test', df_test_normalized_x, df_test_normalized_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec65d714-40e5-4695-a999-985ee62062fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import heapq\n",
    "import time\n",
    "from wordfreq import word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2dadf00-56b1-4fdf-b1dd-af68f286afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_misspelled(word, lan):\n",
    "    return not word_frequency(word, lan) > 0\n",
    "\n",
    "def probabilities_letter(landmark_x, model):\n",
    "    prob_dict = {}\n",
    "    probs = model.predict_proba(landmark_x)\n",
    "    for cls, p in zip(model.classes_, probs[0]):\n",
    "        #print(f\"{cls}: {p:.3f}\")\n",
    "        prob_dict[cls] = p\n",
    "    return prob_dict\n",
    "\n",
    "def find_top_options(captured_probabilities, num_options = 5, max_misspell = 2, include_vocab_freq = True, language = 'en', timeout = 5):\n",
    "    top_letters = []\n",
    "    log_probs_landmarks = []\n",
    "    log_probs_diff = []\n",
    "\n",
    "    #find all possible values for the captured probabilities\n",
    "    for i, probs in enumerate(captured_probabilities):\n",
    "        top_letters_landmark = dict(sorted(probs.items(), key=lambda x: x[1], reverse=True))#[:num_options]) #we need max num_options possibilities per letter\n",
    "        top_letter_landmark = sorted(probs, key=probs.get, reverse=True)[0]\n",
    "        #print(top_letter_landmark)\n",
    "        log_probs_diff.append([])\n",
    "        #print(log_probs_diff)\n",
    "        letters_to_delete = []\n",
    "        for letter in top_letters_landmark:\n",
    "            if probs[letter] == 0:\n",
    "                letters_to_delete.append(letter)\n",
    "            else:\n",
    "                log_probs_diff[i].append((letter, np.log(top_letters_landmark[letter]) - np.log(top_letters_landmark[top_letter_landmark]))) \n",
    "        for letter in letters_to_delete:\n",
    "            del top_letters_landmark[letter]\n",
    "\n",
    "    \n",
    "    #find all possible words in descending order of likeliness\n",
    "    # initial state: pick the best (0) from each list\n",
    "    start_indices = tuple([0]*len(log_probs_diff))\n",
    "    start_choice = [log_probs_diff[i][0] for i in range(len(log_probs_diff))]\n",
    "    start_sum = sum(val for _, val in start_choice)\n",
    "\n",
    "    start_word = ''.join(char for char, _ in tuple(log_probs_diff[i][0] for i in range(len(log_probs_diff))))   \n",
    "\n",
    "    # max-heap (negate sums because heapq is min-heap)\n",
    "    heap = [(-start_sum, start_indices, 0)]\n",
    "    seen = {start_indices}\n",
    "\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    run_time = 0\n",
    "\n",
    "\n",
    "    while heap and len(results) < num_options and run_time <= timeout:\n",
    "        \n",
    "        #print(new_time - start_time)\n",
    "        neg_sum, idx_tuple, changes = heapq.heappop(heap)\n",
    "        current_sum = -neg_sum\n",
    "        # build the actual combo\n",
    "        combo = tuple(log_probs_diff[i][idx_tuple[i]] for i in range(len(log_probs_diff)))\n",
    "        \n",
    "        word = ''.join(char for char, _ in combo)\n",
    "        num_misspell = sum(1 for _, num in combo if num != 0)\n",
    "        \n",
    "        if not is_misspelled(word, language) and num_misspell <= max_misspell:\n",
    "            #print(word, \"has been found with\", num_misspell, \"differences.\")\n",
    "            word_prob = np.exp(current_sum) * word_frequency(word, language)\n",
    "            results.append((word, word_prob))\n",
    "\n",
    "        # push neighbors: increment one dimension (if possible)\n",
    "        for dim in range(len(log_probs_diff)):\n",
    "            j = idx_tuple[dim]\n",
    "            if j + 1 < len(log_probs_diff[dim]):\n",
    "                nxt = list(idx_tuple)\n",
    "                nxt[dim] = j + 1\n",
    "                nxt = tuple(nxt)\n",
    "\n",
    "                if nxt in seen:\n",
    "                    continue\n",
    "\n",
    "                new_changes = changes + (1 if j == 0 else 0)\n",
    "                if new_changes > max_misspell:\n",
    "                    continue\n",
    "                    \n",
    "                # compute new sum efficiently\n",
    "                old_letter, old_val = log_probs_diff[dim][j]\n",
    "                new_letter, new_val = log_probs_diff[dim][j+1]\n",
    "                heapq.heappush(heap, (-(current_sum - old_val + new_val), nxt, new_changes))\n",
    "                seen.add(nxt)\n",
    "        new_time = time.time()\n",
    "        run_time = new_time - start_time\n",
    "    if run_time > timeout:\n",
    "        print(\"Time limit of {} seconds reached.\".format(timeout), \"{} word(s) found\".format(len(results)), \"for original {}.\".format(start_word))\n",
    "\n",
    "    return start_word, sorted(results, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f776a2a3-16bb-425f-88d8-25514aedcb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "H\n",
      "A\n",
      "T\n",
      "C\n",
      "Captured word: WHATC\n",
      "Start word: WHATC\n",
      "Improved words:\n",
      "WRATH has probability 0.0000000189\n",
      "WHANG has probability 0.0000000025\n",
      "WHATD has probability 0.0000000002\n",
      "WHATA has probability 0.0000000000\n",
      "WRANG has probability 0.0000000000\n",
      "WHITH has probability 0.0000000000\n",
      "WPATH has probability 0.0000000000\n",
      "WHETH has probability 0.0000000000\n",
      "WHATE has probability 0.0000000000\n",
      "WUSTL has probability 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "frame_width = int(cam.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "landmarker = mp.tasks.vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "captured_probabilities = []\n",
    "captured_chars = []\n",
    "probabilities = {}\n",
    "prediction = None\n",
    "\n",
    "def store_char_from_mouse_click(event,x,y,flags,param):\n",
    "    global prediction\n",
    "    global probabilities\n",
    "    global captured_chars\n",
    "    global captured_probabilities\n",
    "    if event == 1:\n",
    "        if prediction is not None:\n",
    "            print(prediction)\n",
    "            captured_chars.append(prediction)\n",
    "            captured_probabilities.append(probabilities)\n",
    "\n",
    "cv2.namedWindow('Camera')\n",
    "cv2.setMouseCallback('Camera', store_char_from_mouse_click, (captured_probabilities, captured_chars, probabilities, prediction))\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    detect_async(frame, landmarker)\n",
    "    frame = draw_landmarks_on_image(frame, lm_result)\n",
    "    \n",
    "    if lm_result and lm_result.hand_landmarks != []:\n",
    "        data = store_result(lm_result, None, True)\n",
    "        data.pop('clazz', None)\n",
    "        probabilities = probabilities_letter(pd.DataFrame([data]), nn_model)\n",
    "        prediction = max(probabilities, key=probabilities.get) \n",
    "        if probabilities[prediction] > 0.7:\n",
    "            cv2.putText(frame, prediction, (250, 70), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 4)\n",
    "    cv2.imshow('Camera', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('s'):\n",
    "        print(prediction)\n",
    "        captured_chars.append(prediction)\n",
    "        captured_probabilities.append(probabilities)\n",
    "    elif cv2.waitKey(1) == ord('q'):\n",
    "        break    \n",
    "\n",
    "close(landmarker)\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if len(captured_chars) > 0:\n",
    "    print(f'Captured word: {\"\".join(captured_chars)}')\n",
    "\n",
    "    if len(captured_chars) > 3:\n",
    "        start_word, improved_words = find_top_options(captured_probabilities, num_options = 10, max_misspell = 3, language = 'en', timeout = 10)\n",
    "        print(f'Start word: {start_word}')\n",
    "        print('Improved words:')\n",
    "        for alternative_word, probability in improved_words:\n",
    "            print(alternative_word, \"has probability {:.10f}\".format(probability))\n",
    "    else:\n",
    "        print('To few characters to predict corrections')\n",
    "else:\n",
    "    print('No characters captured')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
